These are the Julia files necessary to reproduce the experimental results.

===== METHOD FILES =====

  [crowdbt.jl] contains our implementation of Chen et al.'s Crowd-BT algorithm

  [odonovan.jl] contains our implementation of O'Donovan et al.'s algorithm.
      Because O'Donovan et al.'s method is equivalent to Bradley-Terry-Luce 
      when the weights are ignored, we use this same code for BTL.


===== EVALUATION FILES =====

  [nll_of_scale.jl] contains the code to calculate the negative log-likelihood
      of a scale with an optional user weight file. This is used to compute the
      left columns of Table 4.
 
 [compute_tau_for_fonts.py] is used to compute the tau between different orders
      of fonts. It is used to compute the far right columns of Table 4.



===== HELPER FILES =====

 [data_loading.jl] contains helper methods used to load the data for both
      crowdbt.jl and odonovan.jl

 [negative_log.jl] contains helper methods for calculating the sigmoids and
      negative log values.


===== SIMULATION =====

  We provide the code necessary to create the data for the simulation
  (Section 6). Because of the storage requirements, we provide a few sample
  files, which should aid any interested party in verifying that their input
  and results match ours.

  The folder [data] contains sample files. We provide examples of both the SAME
  and CONVENIENCE sampling methods for omega = 0.1. Input files are found by
  expanding:

    [input/simulation/SAMPLING/even/WIDTH/JUDGE_COUNT/SEED/seed_SEED_count_COUNT_per_pair_JUDGE_COUNT.arrow.lz4]

  where:
    SAMPLING is either "same" or "convenience"
    WIDTH is the omega value * 100 (e.g., 10 means omega = 0.1)
    JUDGE_COUNT is the number of judges per pair (e.g., 8)
    SEED is the random seed value (e.g., 713)
    COUNT is the number of pairs (e.g., 800)

  The files [data/SAMPLING/even/WIDTH/JUDGE_COUNT/SEED/seed_SEED_count_COUNT_user_summary.csv] 
  provide the number of pairs assigned to each judge as well as the number of
  judgments the judge makes that do not agree with the s* order. However,
  recall that the disagreements with s* are entirely a product of random
  sampling according to Eq. 1.

  To run the simulation:

    1. Create the decision data using [create_same_users.jl]. This generates
       the input for the "same" allocation.

      For example, the command

        julia generate_same_users.jl --seed 713 --pair-counts 800 --width 10

      should create a new folder [data] with input files that match those in
      [input/simulation/data], the provided sample.  Note that the provided
      sample data contains summaries for all of the seeds and counts, but the
      generated one will be just the requested input.

    2. Generate the convenience sample from the same sample to simulate
       crowdsourced judges.

      For example, the command

        julia generate_convenience_from_same.jl --seeds 713 --pair-counts 800 --width 10

      should create a new folder containing the convenience samples derived
      from the same samples. This process may take some time.

      For both [generate_same_users.jl] and 
      [generate_convenience_from_same.jl], one can use the flag
      "--users-per-pair" to restrict iteration.

      The summaries for the convenience sample are not generated by default.
      This is because the summary should be identical to the summary for the
      corresponding sample using the same samples since the votes do not
      change. However, to verify the result, one can compute the summary using:

        julia create_summaries.jl --seeds 713 --pair-counts 800 --width 10 --series convenience

    3. Run the simulation using [odonovan.jl] or [crowdbt.jl]

    NOTE: One often wants more data about the judges to better analyze the
          results.

    4. Create user summaries better to understand the vote distribution in the
       convenience sample.

        julia compute_user_summaries.jl  --seeds 713 --per-pair 8 --pair-counts 800 --width 10 --series convenience

      Please note: for this file, one will need to specify the per-pair or it
      will default to just 8.

      One can also create user summaries for the series using the "same"
      assignment method, but it will not be particularly interesting since
      every judge rates every pair.

  
  NOTE: The input data for the simulation shown in the paper is around 85 GB.
        The output will use around another 3-5 GB if transcripts of the IPOPT
        output are saved.  We ran the simulation using an HTCondor cluster with
        greater than 1500 cores, mostly 2.4 GHz or better Intel Xenons
        (e.g., E5530, X5355, X5440).  The computation time depends on a large
        number of factors, but expect the equivalent of several CPU years. 
        Memory requirements can also pose a challenge, particularly when
        solving for the large number of pairs with Crowd-BT.


===== REMARKS =====

Note that the paper uses the term JUDGES, but the source files may refer to the
decision makers as either USERS or PARTICIPANTS.  These terms were incorporated
early in the development cycle and have not been updated to match the paper
terminology.